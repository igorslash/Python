from transformers import AutoTokenizer, AutoModel
import torch
from typing import Dict, Any, Optional

_MODEL_CACHE = {}


def get_embeddings(text: str, model_name: str = "bert-base-uncased") -> Optional[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é BERT."""

    # ‚Üê –î–û–ë–ê–í–ò–õ–ò –ü–†–û–í–ï–†–ö–ò
    if not text or not text.strip():
        print("‚ùå –û—à–∏–±–∫–∞: –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç")
        return None

    try:
        if model_name not in _MODEL_CACHE:
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {model_name}...")
            _MODEL_CACHE[model_name] = {
                'tokenizer': AutoTokenizer.from_pretrained(model_name),
                'model': AutoModel.from_pretrained(model_name)
            }

        tokenizer = _MODEL_CACHE[model_name]['tokenizer']
        model = _MODEL_CACHE[model_name]['model']

        inputs = tokenizer(text, return_tensors="pt", truncation=True,
                           padding=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs)

        return {
            "embeddings": outputs.last_hidden_state.tolist(),
            "shape": list(outputs.last_hidden_state.shape),
            "model": model_name,
            "text_length": len(text)
        }

    # ‚Üê –î–û–ë–ê–í–ò–õ–ò –û–ë–†–ê–ë–û–¢–ö–£ –û–®–ò–ë–û–ö
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        return None


# –¢–µ–ø–µ—Ä—å –∫–æ–¥ —É—Å—Ç–æ–π—á–∏–≤ –∫ –æ—à–∏–±–∫–∞–º!
result1 = get_embeddings("")  # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç ‚Üí –æ—à–∏–±–∫–∞
result2 = get_embeddings("Hello world")  # –£—Å–ø–µ—Ö
result3 = get_embeddings("x" * 1000)  # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –æ–±—Ä–µ–∂–µ—Ç—Å—è