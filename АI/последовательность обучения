Давай отполируем твою схему до блеска (изменим
только порядок Dataset и DataLoader):

Данные (Текст) → попадают в Dataset.

Dataset использует Vectorizer, чтобы превратить
текст в Тензоры (цифры).

DataLoader (надстройка над датасетом) забирает
эти тензоры и пакует их в Батчи.

Classifier (Модель) получает батч, считает
ответ и сравнивает с правильным.

Loss (Ошибка) → Backpropagation (откатка/обратный ход)
→ Оптимизатор подкручивает веса.

Повтор, пока модель не станет «умной».


batch = next(iter(dataloader)) — «Грузчик» привез одну пачку (батч)
со склада.
outputs = model(batch['x_data']) — «Судья» (Модель)
посмотрел на цифры и выдал свой вердикт (логиты).
loss = loss_func(outputs, batch['y_target']) — «Учитель»
сравнил вердикт модели с правильным ответом из батча
и выставил штраф (ошибку).
loss.backward() — Та самая «откатка». PyTorch
пробегает назад по всей сети и считает, насколько
виноват каждый вес (слово) в этой ошибке.
optimizer.step() — Команда «крути ручки!». Оптимизатор
реально меняет числа в весах, чтобы в следующий
раз ошибка стала меньше.

